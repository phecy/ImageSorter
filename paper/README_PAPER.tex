\documentclass{article}
\usepackage{spconf,amsmath,epsfig}
\usepackage[caption=false]{subfig}
\usepackage{epstopdf}

\pagestyle{empty}
\begin{document}\sloppy
\topmargin=0mm			

%%

\title{
Selective Removal of Extraneous Photographs
}

\name{Anonymous submission}
\address{}
%\name{K. Armin Samii, Allison Carlisle, Uliana Popov, James Davis}
%\address{[ksamii,acarlisl]@ucsc.edu,[uliana,davis]@soe.ucsc.edu}

\maketitle	
\begin{abstract}
In this paper, we propose a method for selecting the most representative high-quality images from a set of user photographs. To avoid redundancy arising from many similar images, we find all sets of near-duplicates. We then rank images based on three technical qualities: exposure, blur, and contrast. Finally, we provide an ordering of the images which accounts for both quality and uniqueness. Our goal is not to rank images based on subjective aesthetic qualities, but instead to help a photographer filter out technically flawed photographs and focus on objectively high quality shots.
\end{abstract}	

\begin{keywords}
image processing, quality screening, exposure, meta data, color content  %image colour analysis , image segmentation , meta data , camera blur , digital albuming , quality screening , exposure , color content
\end{keywords}

\section{Introduction}
\label{sec:intro}
%With the ease of digital cameras, both amateur and professional photographers take more pictures than they will need. An organized user will delete excess photos and keep the most important ones. We need something like: The digitalization has led to an ever increasing size of personal photo collections. Today, consumers often take hundreds or even thousands of pictures of an event.

The ease of using digital cameras allows for a large number of photographs to be taken at any given event. This increase in photograph quantity can lead to many unwanted low-quality photographs that need to be filtered by the user, which can be time-consuming and repetitive.

To automate this process, an aesthetics ranking technique could be implemented to find the appeal each image has, but this method would discard many images with potential to be fixed through retouching.

%So, a technical quality ranking technique can implemented, and the best images would be kept and the worst discarded. This method solves the previous problem, but does not take into account the redundancy of similar images, and the user would be left with a set which does not span their entire photo set.

Implementing a technical quality ranking technique will keep the photographs suited for retouching, but does not take into account redundancy of similar images, and the user would be left with a set which either includes many duplicates or does not span the entire event.

%To fix this, a similar image detector could be implemented giving the user the best image from every subset of similar images. This is still not ideal because it would leave the user with too many low-quality photographs that, although unique in the set, are not worth keeping.

We thus propose a technique which ranks images based on their technical qualities with a bias for unique images.%, but without forcing uniqueness by selecting the best from each set.

To focus our research and determine which image qualities are relevant to technical image quality, we assume three general steps a photographer takes between shooting and using a picture, which we call the Photographer's Process:
\begin{enumerate}
\item Remove: Sort through the imported images and remove the ones least suited for retouching.
\item Retouch: Modify the raw files just selected to stylize and enhance them.
\item Retrieve: Select the retouched images most suitable to a given task.
\end{enumerate}

This work focuses on the removal stage and contains both quality assessment and similarity ranking. We implement technical quality assessment using three modules: blur detection, exposure, and color harmony. The blur detection uses three measures: the gradient magnitude of edges, the similarity to a predicted out-of-focus image, and the probability of a linear point-spread function existing. Exposure is measured by finding the balance in brightness throughout the image, both locally and globally. Contrast is measured based on the local changes in brightnesst throughout an image. We implement similarity ranking using global color similarity, foreground color similarity, and timestamps.  These modules are combined to obtain a reordering of the set representing how suited they are for removal (the "Importance Order"). A user study has shown XX\% accuracy in our absolute ranking of photographs. The similarity ranking was evaluated separately and found to have XX\% accuracy. Fig. \ref{fig:Examples} shows our accuracy in finding the subject and rating quality.% 86% comes from our average difference to turk ratings on Ke's set = 1.35

\section{Related Work}

Previous works have classified images as professional vs. amateur with high accuracy\cite{1640788}\cite{springerlink:10.1007/11744078_23}\cite{springerlink:10.1007/978-3-540-88690-7_29}, which is useful for search engines' retrieval of high quality images. Others have focused on personalized \emph{aesthetic} rankings of photographs\cite {Sun:2009:PAB:1631272.1631351}\cite {Yeh:2010:PPR:1873951.1873963}. All of these works focus on finalized images, while we focus on images that are still raw and may be retouched afterward.

Event classification has been explored in order to choose the best images from an entire event\cite{1223566}\cite{4444209}, while we choose the best images from each scene within each event: images with the same subject content rather than the same event context.

Kormann, Dunker, and Paduschek\cite{springerlink:10.1007/978-3-642-10543-2_23} describe a method of automatically rating and ranking images based on image content and time-metadata, but do not report numerical results, only that their results are better than random. 

%Various publications have focused on improving the quality of an image, which is the second step of the Photographer's Process\cite{Bhattacahrya:2010:FPA:1873951.1873990}\cite{Kopf:2008:DPM:1409060.1409069}.

%Others deeply researched assessing quality for the purposes of photograph retrieval from a web or multi-contributor set \cite{Yeh:2010:PPR:1873951.1873963}\cite{vanZwol:2010:FEI:1772690.1772788}\cite{springerlink:10.1007/978-3-642-17187-1_57}\cite{Cui:2008:RTG:1459359.1459471}\cite{Chu2010256}.

%Previous works in image quality assessment have ranked image quality based on relevancy and quality
%%blur \cite{springerlink:10.1007/978-3-540-77409-9_26}, exposure \cite{5540170}, color harmony \cite{COL:COL5080160410}\cite{COL:COL10004}, and duplicate detection \cite{Chu2010256}. %User tags on Flickr and online search engines have provided sets of similar images to compare\cite{Berg:EECS-2007-13}\cite{Chu2010256}.
% Duplicate detection isn't quality assessment though?

%%Several papers have developed methods for separating professional and amateur photographs \cite{springerlink:10.1007/978-3-540-30541-5_25}\cite{springerlink:10.1007/11744078_23}\cite{1640788}\cite{springerlink:10.1007/978-3-540-88690-7_29}. %Unlike our work, these assume a finalized image.

%The methods for image retrieval are not as useful for image removal because they assume a finalized photograph, rather than looking at the potential to be enhanced- or, more simply, they focus on aesthetic quality over technical quality. Removal of technically flawed photographs, as opposed to aesthetically unpleasing ones, is more readily automated without consideration of personal preference. We consider color harmony a technical aspect because it can be indicative of probems with lighting and color balance.

%%Compressed image quality assessment has been heavily researched\cite{477498}\cite{1038064}\cite{1284395}, but we assume a high quality import. As described in \cite{1315222}, a photograph's EXIF data can be used to robustly classify images into various semantic categories. Various aesthetic-based assessments focus on exploring human visual attention and sensitivity to photograph content\cite{Sun:2009:PAB:1631272.1631351}\cite{1518955}\cite{Pimenov_fastimage}, demonstrating the viability of content segmentation for image assessment.

\section{Quantifying Image Quality}
\begin{figure*}
  \centering
    \epsfig{file=flowchart.eps,width=16cm}
  \caption{Interest points are used to find the foreground subject. Blur detection, color harmony, and exposure algorithms calculate an image's quality rating. The color distribution of the local foreground subject, and global image, coupled with the timestamp, cluster images into groups. A reordering of the input results, allowing the user to remove images at the end of this ordering.}
  \label{flowchart}
\end{figure*}

To find the Importance Order of the user's photographs, we use both quality assessment and similarity clustering. Quality is determined by the rankings of the blur, exposure, and color harmony modules. Similarity of images is determined by matching the color content and timestamp of every pair of images. Both modules make use of a foreground region detection module based on interest points. We combine these measures to get the final ordering (Fig. \ref{flowchart}).

The quality modules (blur, exposure, and color assessment) provide a ranking between zero and nine, with larger numbers indicating higher quality. Because an image which is flawed in any of the three factors would be considered poor, we propose a formula which penalizes low scores more than it rewards high scores:
\begin{eqnarray}
\left(\displaystyle\sum\limits_{i=1}^nW_i\left({Q_i+t}\right)^\frac{2}{3}\right)^\frac{3}{2}
\end{eqnarray}

\(W_i\) is weight of module \(i\), \(n\) is number of modules, \(Q_i\) is  module \(i\)'s rating for the image, and \(t\) is a leniency threshold to balance each module's output, empirically chosen to be 4. Weights are assigned as follows: blur 60\%, exposure 30\%, and color harmony 10\%.

\subsection{Foreground detection}\label{ContentRecognition}
Several modules make use of foreground detection. We use a Harris Interest Operator (utilizing NASA's Vision Workbench\cite{vision-workbench}). Interest points are obtained for each image. To extract a bounding box from these points, the most dense rectangle is calculated by maximizing the ratio of interest points to rectangle area. We assume this to be the primary foreground subject.
%Although there are more accurate methods available\cite{5649226}, our work only requires this simpler method which provides a bounding box around the most salient foreground object.


\subsection{Exposure}
%\begin{figure}
%  \centering
%    \epsfig{file=imageluminosity.eps,width=8.5cm}
%    \caption{The exposure process: This figure shows the inital segmentation of images by mean luminosity, then the further segmentation of images of medium and high quality (mean ranges 100-120 and 120-138 respectively).}
%    \label{exposurefigure}
%\end{figure}
Exposure is a measure of how appropriate the lighting is in a given image. While easily measured in-camera using a light meter, we must evaluate the lighting condition based on pixel intensity.% Images are ranked according to various measures of their brightness. %The calculation for brightness used is
%\[
%p=.59r+.3g+.11b
%\]
%where \(p\) is the perceived intensity, and \(r\), \(g\), and \(b\) are the red, green, and blue components, respectively(XX THIS FORMULA NEEDS REFERENCE).

%We have determined eight measures by examining a set of images with various exposure problems. 

To assess an image's exposure quality, we use both global and local calculations to compare the average grayscale value of the region to 50\% gray (halfway between black and white).

The global method ranks an image highly when the average grayscale value is near 50\% gray.

The local method divides the image into 20x20 grid. Images are ranked highly when the bright and dark grid squares average near 50\% gray, with no square being extremely right or dark. We also penalize an image if there is not enough variation between grid squares.

The two measures are averaged to provide a final ranking.
   


%The measures relevant to exposure quality are: clipping, highlights, lowlights, the upper 60th and 98th percentiles, the lower 60th and 98th percentiles, and variance. Each of these measures is calculated on both the subject and the background of the image, relative to the area over which it is calculated.

%Based on the idea that the mean value of image brightness is an approximate indication of how well-exposed an image is, 
%Images are first segmented based on their mean brightness value ("mean"). The categories for the mean correspond roughly to a parabolic mapping of exposure values. These divisions, as shown in Fig. \ref{exposurefigure}, are used as the starting point of analysis. %, with well exposed photographs having a mean value between 130 and 140. 
%The quality decreases as 
%The further towards the extreme high and low means, the worse the quality of the image is. %Within each category, different measures indicate either a positive or negative overall impact on image quality, which impact depends upon the various categories. For example, in an overall dark mean, having more extreme bright areas makes the image more balanced, while in a bright image they usually indicate that it has been overexposed.

%Overexposure of an image can lead to large areas in the photo in which the human eye cannot discern any form (note that this definition can also indicate areas of a solid white value that contains absolutely no data about the form present). However, as the program is primarily concerned about how humans perceive images, the number of pixels in the highest five perceived values are used to calculate the “highlights” present in the image.

%Lowlights can be caused by underexposure, but they can also be a product of poor lighting (e.g., photographing a shadowed object in extremely bright conditions). Again, some lowlights are desirable for contrast in an image, but too much leads to an undecipherable image.

%Clipping is an indication of how much information loss there is in the image. %due to extreme shadows and bright spots, measured as a sum of the Highlights and Lowlights.

%The percentiles of the image are determined for both the bright and the dark sides of brightness. For example, to calculate the lower 60th percentile, the value is found at which the sum of the number of pixels that are darker than the given value is equal to sixty percent of the pixels in the image. The upper percentiles are found by the summation of the pixels that are brighter than the given value. These four measures give a good indication of the spread of the brightness, e.g., how sharp the transitions between the extreme and middle values are.

%The most extreme mean brightness values are images of very poor quality, and are rated as such based solely on the mean value. Means between 80 and 170 encompass nearly all images of acceptable to excellent exposure, and thus require more analysis. They first are divided into bins according to their mean value, then again divided by the saturation value.

%We use the mean value, categories, subcategories, and the eight measures to determine if the exposure is balanced throughout the image. An image with a dark mean should have bright pixels and vice versa. An unbalanced image is rated poorly, even if it has an acceptable mean. Well-balanced images within the acceptable mean range are rated highly.


\subsection{Blur Detection}
There are three measures used to assess the amount of blur: edge width, global point-spread, and a comparison against a computed blurry image.

The first two rely on an edge-detection algorithm which is done in three steps. First, we calculate the differences between a pixel's luminosity and the image's average luminosity. Large differences are considered edge pixels. Mislabeled edge pixels are then removed by ensuring each one is connected to another edge pixel. Finally, edges are spaced out by ensuring there is a large radius of non-edge pixels around each set of edge pixels; if there is not, the edge pixel with the largest value from the first step is kept. % XX FIGURE?

The first measure finds the width of each edge by finding a "line of maximum contrast" from the brightest to darkest pixel around each edge. Sharper edges will have shorter line lengths.

The second measure handles looks at the orientation of the lines  of maximum contrast. If there are significantly more lines oriented in a single direction than the average, we assume the point-spread to be in that direction. %XX def. needs a figure

Finally, if both of these measures are inconclusive, we compute a predicted blurred model of the foreground using a gaussian blur on the image. The closer an image is to this model, the blurrier it is.

These three values combine to determine the amount of blur. Only one needs to have high confidence to assess the blur. The results are averaged if none have high confidence.%, and it is especially accurate when comparing two similar images. % XX add result here, keep sentence?


\subsection{Contrast}
%\begin{figure}
%  \centering
%    \epsfig[scale=0.38,clip]{colorharmony.eps}
%  \caption{The harmonies described in \cite{COL:COL10004} are used to find the quality of the relationships between colors.}
%\end{figure}

UUU

%To determine the quality of a photograph's color content, we use Cohen's seven types of color harmony\cite{Cohen-Or:2006:CH:1179352.1141933} to determine which, if any, is contained in the photograph. For each image, we find the hue which is most represented across the seven types, as well as the percent of pixels which match any of the harmonies.

%We separate the pixels into bins based on their saturation values, using user-study data from Amazon Mechanical Turk to determine the boundaries of each bin. We then look for the most closely matched harmony type within each bin, ranking the bin based on how far it is from the closest type. We combine the bins' rankings for a final quality ranking.

%The first harmony checked is the average and great image's I type. If there is a large amount of I harmony, the difference between I and Y harmonies is checked. Because I and Y overlap, you need to check how much of an increase if between the two. If it is very significant, Y harmony is most likely. When the difference is slight, you have a good I harmony (and thus high rating). If there is a medium difference, the I harmony is weak. Rating of the images can be determined on medium to great harmony. A similar check is done in X versus Y harmony. A last pass is done to check the L type on the great images, and a pass checking the v and i harmonies is done on the average images.

%The poor images follow a similar path, starting with analysis by the X harmony, and ending with a pass done with i harmony.

\section{Similar-Image Clustering}
To find similar images we use three measures: time nearness, histogram similarity, and a comparison between degraded versions of the photographs.
%\begin{figure}
%  \centering
%    \epsfig[scale=0.07,clip]{similarimages.eps}
%  \caption{We cluster images based on their temporal nearness, overall color distribution, and foreground subject color distribution.}
%\end{figure}

\subsection{Timestamp}
First, we use the timestamp to obtain a time similarity index between all pairs of images. Prior work focused on finding large timestamp gaps\cite{1292402}. We want to allow nonconsecutive images to be grouped together, so we have derived a formula which finds temporal closeness between every pair of images. We find the similarity \(S_{i,j}\) between two images \(i\) and \(j\) on a 0-9 scale:
\begin{eqnarray}
S_{i,j}=\frac{G_{i,j}}{A_{i,j}}
\end{eqnarray}
Where \(A_{i,j}\) is the average gap of the 8 images directly before \(i\) and after \(j\), and  \(G_{i,j}\) is the log of the time gap between the two images taken at time \(T_i\) and \(T_j\):
\begin{eqnarray}
G_{i,j}=\log(|T_i-T_j|)
\end{eqnarray}
The calculated similarity index \(S_{i,j}\) is used to weight the results of the next two steps.


\subsection{Histogram Similarity}
The histogram similarity is evaluated by dividing the image into a 2x2 grid and finding their grayscale, red, green, and blue channel histograms. We use 20 histogram bins for each channel, each of which has overlapping values for resistance against minor exposure changes. We also find the median value of each channel. When comparing the histograms of two images, the bins are scaled so that all channels have the same median value. This provides resillience against larger exposure changes.

To calculate the similarity, the difference in the size of each bin for each channel is summed across the grids. Each grid is weighted by the amount of the foreground that it contains, and the weighted average is the similarity score, with smaller differences indicating high similarity.


\subsection{Degraded-Image Comparison}
We degrade an image to remove specific features in it while maintaining color and shape information. This in general will be similar to the histogram similarity comparison, but catches any false positives caused by similar histograms in dissimilar images.

First, a strong gaussian blur filter is applied to the photograph to remove features. The image is then scaled to make the comparison have less differences in pixel values. When comparing two images, a small pixel-by-pixel difference indicates higher similarity.

\subsection{Clustering}
Clusters are then formed using a Quality-Threshold algorithm. We first put each image into its own group, then iteratively merge the two most similar groups until a minimum similarity threshold is reached. In this way, groups votes for other similar groups, rather than single image voting for other images, resulting in a noise-resistant clustering. This method successfully groups panoramic sets of images together, even when the first image's histogram does not match the last image's. % Add panorama image?


\section{Results}

\begin{figure*}[t!]
\centering
\subfloat[Unsorted]
{
	\fbox{\epsfig{file=result_unsorted.eps,width=8.5cm}}
	\label{resultunsorted}
}
\subfloat[Sorted]{
	\fbox{\epsfig{file=result_sorted.eps,width=8.5cm}}
	\label{resultsorted}
}
\caption{\subref{resultunsorted} shows four sets of similar photographs provided by the user. \subref{resultsorted} shows the reordered set, which correctly matched the four groups together and chose the top image from that set (as voted by Turk users).}
\label{fig:ResultSorting}
\end{figure*}

\begin{figure*}[t!]
\centering
\subfloat[
Blur: 2.4 \(\mid\)
Exposure: 6 \(\mid\)
Color: 4]
{
	\epsfig{file=example_blur.eps,width=5cm}
	\label{example_blur}
}
\subfloat[Blur: 6 \(\mid\)
Exposure: 1.2 \(\mid\)
Color: 1]{
	\epsfig{file=example_expose.eps,width=5cm}
	\label{example_expose}
}
\subfloat[
Blur: 9 \(\mid\)
Exposure: 7.4 \(\mid\)
Color: 7]{
	\epsfig{file=example_good.eps,width=5cm}
	\label{example_good}
}
\caption{Examples of \subref{example_blur}\subref{example_expose} low quality and \subref{example_good} high-quality images. The dots are interest points found; the square is the bounding box considered to be the subject. Despite the too-inclusive box in \subref{example_good}, each algorithm worked properly.}
\label{fig:Examples}
\end{figure*}

We gathered Amazon Mechanical Turk user ratings on our own dataset of 459 sequential images.

%Enumerated below are the results and comparisons to similar works.

%\subsection{Similar Image Detection}
We asked Turk users to group together photographs within four sets of twenty images based on their own personal measure of "similarity." We agree with Turk users' decisions of whether a given pair of images is in the same group with 81\% accuracy.

%\subsection{Image Rating}

%Separate trials were run to compare our algorithm to previously ranked images: 48 from Ke, Tang, and Jing\cite{1640788}, and 100 from Luo and Tang\cite{springerlink:10.1007/978-3-540-88690-7_29}. We then compared 459 of our own images to Turk users. Here we compare against an absolute rating of each image without regard to the sets of similar images.

%Using the 48 image data set from Ke, Tang, and Jing\cite{1640788}, and Amazon Turk ratings as a ground truth, we achieve a correlation coefficient of \(.340\), which is comparable to the authors' correlation coefficient of \(.495\). Similarly, with a random sample of 100 images from Luo and Tang's public data set of high-quality images\cite{springerlink:10.1007/978-3-540-88690-7_29}, we achieve a correlation coefficient of \(.311\). These numbers show that our algorithm provides positive absolute results when comparing photographs independently, despite our focus on ranking images within a set of related images.

%\subsection{Unique, High-Quality Sets}
Fig. \ref{fig:ResultSorting} shows an example of a user's input images being resorted by quality and uniqueness. The top four images in \ref{resultsorted} (as would be chosen by a user) represent the highest quality images in each similar-image set. The nontrivial case of zoomed-in flowers was recognized as the same foreground subject. The top images are consistent with Turk users' votes.

%\subsection{Binary Classification} Using the same data as the Image Rating trials, we have found that our algorithm can distinguish between professional and non-professional images well. Barsky \cite{Yeh:2010:PPR:1873951.1873963} and Luo \emph{et. al}\cite{springerlink:10.1007/978-3-540-88690-7_29} obtained 96\% accuracy when classifying images into the two categories. When classifying "non-professional" as Turk ratings below XX/9 and "professional" above XX/9, our rating system matches a user's with XX\% accuracy.

\section{Conclusion and Future Work}
We propose a method of applying current research to automate another step of the Photographer's Process. By focusing on obtaining an ordering which is representative of all photographs taken, we obtain a diverse set of high quality images similar to what a user would have chosen manually. We derive a novel algorithm for analyzing exposure quality. We improve upon previous algorithms which find temporal gaps between images to obtain a metric for temporal nearness. The final ordering depends on both the quality ranking and the number of similar images which have already appeared.

Our work focuses on a small portion of the Photographer's Process. In the future, we would like to see the idea of relative rankings applied to the second Retouching step. (It has already been extensively applied to the third Retrieval step.) Retouching can use relative processing to increase creativity between similar images or combine data from multiple images. With this, we would be able to automate the Photographic Process.

\section{Project Page}
Our code and data set have been made available online at http://www.artoonie.com/imagesorter

{\footnotesize
 \bibliographystyle{IEEEbib}
 \bibliography{README_BIB}
}
\end{document}
