\documentclass[twocolumn]{article}
\usepackage{graphicx}
\title{
Assessing Image Quality on Import From Camera
} % Work in Progress
\author{
K. Armin Samii\\
Computer Science Undergraduate\\
Univ. of Calif. at Santa Cruz\\
ksamii@ucsc.edu
\and
Allison Carlisle\\
Bioinformatics/Computer Science Undergraduate\\
Univ. of Calif. at Santa Cruz\\
acarlisl@ucsc.edu
\and
Uliana Popov\\
Computer Science Graduate\\
Univ. of Calif. at Santa Cruz\\
uliana@soe.ucsc.edu
\and
James Davis\\
Computer Science Professor\\
Univ. of Calif. at Santa Cruz\\
davis@soe.ucsc.edu
}
\date{\today}
\begin{document}
\maketitle
\begin{abstract}
In this paper, we propose a novel method for ranking images as they are imported from a users camera. By gathering sets of similar images, we can rank images in relation to others in the set and provide the user with the best images from every scene photographed. We filter out images which are technically flawed to provide the photographer with a high-quality representative subset of the imported images. The filtered set chosen by our algorithm matches the user's choices with high accuracy (XX\%).
\end{abstract}

\section{Introduction}
To focus our research and determine which image qualities are relevant, we assume three general steps a photographer takes between shooting and using a picture, which we call the Photographer's Process:
\begin{enumerate}
\item Remove: Sort through the images being imported and select the ones most suited for retouching, removing the rest from consideration.
\item Retouch: Modify the raw files just selected to stylize and enhance them.
\item Retrieve: Select the retouched images most suitable to a given task.
\end{enumerate}
Our research focuses on the first step of the Photographer's Process. By quantifying visual qualities which cause a photograph to be rejected, we have created an automated system which rejects over XX\% of the images a human would reject, based on XX data sets of XX images tested on XX users each. We are also accurate in rating images independently of their set, achieving XX\% of our no-reference ratings being accurate to one standard deviation of the users' ratings on a ten-point scale.

\section{Related Work}
Various publications have focused on improving the quality of an image (the second step of the Photographer's Process)\cite{Bhattacahrya:2010:FPA:1873951.1873990}\cite{Kopf:2008:DPM:1409060.1409069}; more deeply researched is assessing quality for the purposes of photograph retrieval\cite{Yeh:2010:PPR:1873951.1873963}\cite{vanZwol:2010:FEI:1772690.1772788}\cite{springerlink:10.1007/978-3-642-17187-1_57}\cite{Cui:2008:RTG:1459359.1459471}\cite{Chu2010256}. The methods for image retrieval are not as useful for image removal because they assume a finalized photograph, rather than looking at the potential to be enhanced- or, more simply, they focus on aesthetic quality over technical quality. Removal of technically flawed photographs, as opposed to aesthetically unpleasing ones, is more readily automated without consideration of personal preference. We consider color harmony a technical aspect because attempting to color adjustment is a lossy process (XX ALLISON, I DON'T KNOW WHAT I'M TALKING ABOUT, CAN YOU MAKE SOMETHING UP AND FIND A REFERENCE PERCHANCE?)

Previous works in general image quality assessment have explored single features such as blur\cite{springerlink:10.1007/978-3-540-77409-9_26}, exposure\cite{5540170}, and color harmony\cite{COL:COL5080160410}\cite{COL:COL10004}. User tags on Flickr and online search engines have provided sets of similar images to compare\cite{Berg:EECS-2007-13}\cite{Chu2010256}. Several papers have developed methods for separating professional and amateur photographs\cite{springerlink:10.1007/978-3-540-30541-5_25}\cite{springerlink:10.1007/11744078_23}\cite{1640788}\cite{springerlink:10.1007/978-3-540-88690-7_29}. Unlike our work, these assume a finalized image.

Compressed image quality assessment has been heavily researched\cite{477498}\cite{1038064}\cite{1284395}, but we assume a high quality import. \cite{1315222} shows how a photograph's EXIF data can be used to robustly classify images into various semantic categories. Various aesthetic-based assessments focus on exploring human visual attention and sensitivity to photograph content\cite{Sun:2009:PAB:1631272.1631351}\cite{1518955}\cite{Pimenov_fastimage}, showing how content segmentation is useful.

Our work assesses an image based on blur, exposure, and the relationship between colors. We focus on the quality of foreground, and provide a relative ranking within a set of contextually similar images. An absolute rank is also provided, providing robustness in cases where there are no similar photographs to compare to in the input set.

\section{Quantifying Image Quality}
Each selected factor will provide a ranking between zero and nine, with larger numbers indicating higher quality. Because an image which ranks low on any part would be considered poor, we have propose an algorithm which penalizes low scores more than it rewards high scores, because our primarily goal is to weed out poor images. We use an inverse-logarithmic scale for this overall ranking:
\[
\displaystyle\sum\limits_{i=1}^n\frac{\log^{-1}(W_iQ_i)}{r*n}
\]
Where \(r\) is the range of ratings, \(Q_i\) represents the rating of module \(i\), and \(W_i\) represents the weight of that module. Weights are assigned as follows: XX ARE THEY REALLY?
\begin{itemize}
\item \(W_{exposure}=50\%\)
\item \(W_{blur}=40\%\)
\item \(W_{color}=10\%\)
\end{itemize}

XX EACH ITEM BELOW SHOULD HAVE AN IMAGE TO GO WITH IT

\subsection{Content Recognition}
Using NASA's Vision Workbench\cite{vision-workbench}, we obtain interest points and find the most dense region. We assume this to be the primary content in the image. It is used in detecting similar images, determining exposure quality, and calculating blur levels. Although there are more accurate methods available\cite{5649226}, our work only requires this simpler method which provides a bounding box around the most salient foreground object.

\subsection{Similar-Image Clustering}
To find similar images, we perform three steps of increasing complexity for high confidence.
First, we use the timestamp to obtain a similarity index between all pairs of images. Based on the algorithm explained in \cite{1292402}, which finds gaps in timestamps, we have derived a formula which finds temporal closeness between two images:
\[
T_{i,j}=\frac{G}{A_g}
\]
Where \(G\) is the log of the time gap between the two images:
\[
G_{i,j}=\log(|T_i-T_j|)
\]
And \(A_g\) is the average gap within a window:
\[
A_g=\displaystyle\sum\limits_{a={i-w}}^{j+w}G_{a,i}
\]
We have empirically chosen a window size of \(w=8\).
We use the calculated similarity index \(T_{i,j}\) as a weight for the next two steps.

The first content-based step is simple 9-segment test. We divide the images into a 3x3 grid and compare the average color of each square. Because this is sensitive to exposure differences, subject movement, and camera reorientation, we then gather interest points using XX SOME ALGORITHM FROM NASA, OR THE AVERAGE COLOR OF THE ROI. We then compute a ranking based on the number of matched interest points.

The two content-based algorithms, weighted by the timestamp, cluster similar images with high accuracy.

\subsection{Blur Detection}
Using the foreground bounding box, the blur detection algorithm quantifies the contrast between the edges and background. Using the Image Gradient Model proposed in \cite{springerlink:10.1007/978-3-540-77409-9_26}, we obtain a value for the sharpness. To increase accuracy and reduce the adverse affects that acceptable background blur may have, we constrain detection to the bounding box.

Next, we compute the difference in pixel values between the original image and a Gaussian blurred image. This provides little insight on no-reference images, but when looking at the bounding box of similar images, sharper edges results in a larger difference.

XX NEED EMPIRICAL EVIDENCE. COMPARE ORIGINAL IMAGE TO 5mm*{1,2,3} REFOCUSED TO SHOW FOCUS BLUR, AND SOMETHING FOR MOTION BLUR (SIMULATED?).

\subsection{Exposure}
We compare the foreground and background exposure to determine quality. Based on the idea that the the mean value of image luminosity is an indication of how well-exposed an image is, images are segmented based on their average luminosity value.
Luminosity is a measure of how bright the human visual system perceives a color. As the human eye is most sensitive to green and red ranges, those color portions are weighted more heavily to a color's overall perceived intensity. The calculation used by the exposure analyst is
\[
p=.59r+.3g+.11b
\]
where \(p\) is the perceived intensity, and \(r\), \(g\), and \(b\) are the red, green, and blue components, respectively(XX THIS FORMULA NEEDS REFERENCE).

The categories of mean luminosity value (“mean”) correspond roughly to a parabolic mapping of exposure values, with well exposed photographs having a mean of \(130<p<140\), and the further towards the extreme high and low means, the worse quality the image is. These divisions are used as the starting point of analysis. Within each category, different measures will indicate either a positive or negative overall impact on image quality. But the measurements can signify different changes in the various categories. For example, in an overall dark mean, having more extreme bright areas makes the image more balanced, while in a bright image they usually indicate that it has been overexposed.

The other measures of image quality are also based on luminosity, and were determined by taking various measurements on a deliberately chosen pool of images that had a wide variety of exposure problems. The measures are as follows: clipping, highlights, lowlights, the upper 60th and 98th percentiles, the lower 60th and 98th percentiles, and variance. Each of these measures are calculated on both the foreground and the background of the image.

Clipping is an indication of how much information loss there is in the image due to extreme shadows and bright spots. Each of these measures is taken in relation to the total number of pixels being measured.

Overexposure of an image can lead to large areas in the photo in which the human eye cannot discern any form (note that this definition can also indicate areas of a solid white value that contains absolutely no data about the form present). However, as the program is primarily concerned about how humans perceive images, the number of pixels in the highest five perceived values are used to calculate the “highlights” present in the image. Some highlighting can be desirable in an image, but the amount that is considered overexposed varies depending on the image mean.

Lowlights can be caused by underexposure, but they can also be a product of poor lighting (eg, photographing a shadowed object in extremely bright conditions). Again, some lowlights are desirable for contrast in an image, but too much leads to an undecipherable image.

The percentiles of the image are determined for both the bright and the dark sides of luminosity. For example, to calculate the lower 60th percentile, the value is found at which the sum of the number of pixels that are darker than the given value is equal to sixty percent of the pixels in the image. The upper percentiles are found by the summation of the pixels that are brighter than the given value. These four measures give a good indication of the spread of the luminosity, eg, how sharp the transitions between the extreme and middle values are.

The most extreme mean luminosity values are images of very poor quality, and are rated as such based solely on the mean value, although having a bright 98th high percentile can provide enough contrast to make a discernible image, thouugh certainly nothing of quality. Means between 80 and 170 encompass nearly all images of medium through excellent exposure, and thus require the most analysis. Means below 100 are also part of the “low quality” batch of images, they have a fairly clear relationship with the 98th high percentile and are thus analyzed very simply.

Means between 100 and 120 are further subdivided by the amount of highlighting present. XX WRITE SOME MORE STUFF

Means between 120 and 138 were found to have the highest concentration of high ratings. XX WRITE SOME MORE STUFF

The images on the most extreme end of the bright scale were not very populated in the training image set, but were hypothesized to follow a similar curve as the dark side, with the redeeming quality being the presence of dark values.

\subsection{Color Harmony}   
\begin{figure}[t]
  \centering
    \includegraphics[scale=0.38,clip]{colorharmony.eps}
  \caption{The harmonies described in \cite{COL:COL10004} are used to find the quality of the relationships between colors.}
\end{figure}

Based on the seven types of color harmonies described in \cite{Cohen-Or:2006:CH:1179352.1141933}, the "ColorCritic" module measures which types of harmony are present in a photograph. It uses the I,v,L,I,Y, and X type harmonies (T and N are omitted).

For each harmony type, the hue is found with the most occurrences of supporting harmony (other pixels that fall in the harmonic range) as well as the percent of the image that has harmonious pixels. Similar to the Exposure Analyst, ColorCritic divides it's images into several categories before further analysis. However, because people tend to like colorful, intense images (XX THIS NEEDS TO BE SUPPORTED WITH A REFERENCE- IF THE SAME AS COHEN, WE CAN JUST CITE THAT), ColorCritic divides the images by the color saturation in the image using the results from Mechanical Turk rankings to determine where the exact divisions are. The harmony types are used to differentiate between various qualities of images withing the saturation ranges.

The first harmony checked is the average and great image's I type. If there is a large amount of I harmony, the difference between I and Y harmonies is checked. Because I and Y overlap, you need to check how much of an increase if between the two. If it is very significant, Y harmony is most likely. When the difference is slight, you have a good I harmony (and thus high rating). If there is a medium difference, the I harmony is weak. Rating of the images can be determined on medium to great harmony. A similar check is done in X versus Y harmony. A last pass is done to check the L type on the great images, and a pass checking the v and i harmonies is done on the average images.

The poor images are less well-defined but, the X harmony was the best place to start, and then a pass was done with i. XX ALLISON REWRITE THIS PARAGRAPH

\section{Results}
We have run our algorithm on several publicly available datasets as well as our own. We used Amazon Mechanical Turk and gathered a total of 454 (XX  ADD QUALITY FILTER) users rankings across trials.

Enumerated below are the results and comparisons to similar works.

\subsection{Quality Filtering} When asking a user to choose which images to keep and which to discard, our algorithm correctly discarded XX\% of the images, and incorrectly discarded XX\%. When limiting both the algorithm and the user to discarding a fixed number of chronological photographs (ten out of twenty), we improved these numbers to XX\% accuracy with XX\% false-negatives. (XX WE PROBABLY ONLY HAVE TIME TO GATHER THE SECOND NUMBER- FIXED NUMBER OF DISCARDS)

\subsection{Image Rating} Separate trials were run to compare our algorithm to previously ranked images: 48 from \cite{1640788}, 100 from \cite{springerlink:10.1007/978-3-540-88690-7_29}, and 459 from our own data (XX DIDN'T WE DECREASE THE SIZE OF THIS? WE DON'T HAVE JULIE'S, ONLY ULA'S). Here we obtain an absolute, no-reference rating of each image.

We took a random sample of 100 images from the 11,981 images provided by \cite{springerlink:10.1007/978-3-540-88690-7_29}. We asked 44 Turk users to rank 50 images on a scale from one to ten.

\begin{figure}
  \centering
    \includegraphics[scale=0.40,clip]{ke_vs_us.eps}
  \caption{X-axis: ten-point scaled rating.  Y-axis: number of images with this rating. We see that Ke's ratings do not match our ground truth. XX INACCURACY ASIDE, WE SHOULD MAKE THIS WORK FOR B\&W AND MAYBE LABEL AXES ON ACTUAL CHART?}
  \label{fig:ke_vs_us}
\end{figure}
When comparing Ke's data set, we see that their ground truth is based on different factors (Fig.~\ref{fig:ke_vs_us}). Whereas our work matches Amazon Mechanical Turk users' ratings with an accuracy of 80\%, Ke's only matches it with an accuracy of 45\%. Here, accuracy is defined as being within one standard deviation of the users' rankings.

XX INSERT DATA FOR OTHER DATA SET

\subsection{Binary Classification} Using the same data as the Image Rating trials, we have found that our algorithm can distinguish between professional and non-professional images well. Barsky \cite{Yeh:2010:PPR:1873951.1873963} and Luo \emph{et. al}\cite{springerlink:10.1007/978-3-540-88690-7_29} obtained 96\% accuracy when classifying images into the two categories. When classifying "non-professional" as ratings below XX and "professional" as ratings above XX, our rating system matches a user's with XX\% accuracy.

\section{Conclusion and Future Work}
We propose a method of applying current research to automate another step of the Photographer's Process. By ranking an image's quality in relation to other images in the set, rather than an absolute scale, we obtain an accurate sorting of images. We improve upon past research which provides an absolute ranking by applying it to a relative scale. From this, we derive novel reference-based algorithms for exposure analysis, temporal closeness, and blur detection. To ensure robustness when there are no similar sets (and thus no references), images are sorted both relatively within the set of similar images and globally within the set of all input images.

Our work focuses on a small portion of the Photographer's Process. In the future, we would like to see the idea of relative rankings applied to the second Retouching step. (It has already been extensively applied to the third Retrieval step.) Retouching can use relative processing to increase creativity between similar images or combine data from multiple images. With this, we would be able to automate the Photographic Process.
\bibliographystyle{plain}
\bibliography{README_BIB}
\end{document}
