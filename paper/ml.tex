\documentclass[11pt,letter]{article}

\setlength{\topmargin}{0in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.5in}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{appendix}

\usepackage{epsfig}
\title{A Framework for Personalized Photograph Quality Assessment}
\author{
K. Armin Samii \\
ksamii@ucsc.edu
\and
Uliana Popov \\
uliana@soe.ucsc.edu}
\date{May 13, 2011}

\begin{document}
\maketitle
\begin{abstract}
Photograph quality assessment can help aid home users in managing their personal collections in several ways, including deleting low-quality images and browsing for high quality images. Recent research has aimed to automatically rate image quality, but such methods are not yet perfected. In this paper, we propose a framework for quickly developing and testing new features to accelerate the progress of this research. We have two types of features* UUU (the note of terminology can go to the footer of the page, and will be referenced here using *): \textit{high-level features} that are subjective qualities which humans can easily answer UUU: (not sure that "answer" is the best verb, maybe qualify/determine/recognize or something like that), like saturation, exposure or blurriness; \textit{low-level features} that are objective calculations which are produced algorithmically, like edge width or number of dark pixels. We use machine learning techniques to learn how each of these relate to image quality. Our model allows for a developer to easily train new features (both low-level and high-level), UUU (make a stronger relationship between the second half of this sentence and the rest of the abstract. who is this end user, what personaliziation, why? i'd say that for default assesment, when we just want to rank images, we calculate these high and low features, but we also allow a user, in our case photographer, to affect the final decision of the algorithm, aka personalize it )and for an end-user to personalize which high-level features are important.

\end{abstract}

\section{Problem Statement}
Before discussing the details of our solution, we present our problem in full. UUU: (<- this sentence can be removed)

A photographer wishes to automatically rate his photograph collection based on his personal preferences. There are several papers dedicated to solving this problem using machine learning\cite{springerlink:10.1007/11744078_23}\cite{springerlink:10.1007/978-3-642-10543-2_23}\cite{Yeh:2010:PPR:1873951.1873963}. To accelerate research in the area, we need to develop a framework for developers (researchers) to quickly train new features extracted from photographs.

There are two audiences we are trying to serve in this work: (1) the photographer using the application, and (2) the developer producing the application. In other words, we want to make it as easy as possible for the developer to serve the photographer.

To serve (1) the photographer, we want to allow him to choose which features he personally finds important (see Section \ref{abstraction}). To serve (2) the developer, we want to allow her to be able to add new features with ease (see Section \ref{easeofprogramming}). Figure \ref{fig:flowchart} visualizes these ideas.

A note on terminology: We make the distinction between \textit{low-level features} (ones that are computed, such as average edge width) and \textit{high-level features} (ones that are solely for human interpretation, such as blurriness).

\begin{figure*}[b!]
  \centering
    \epsfig{file=mlflowchart.pdf,width=14cm}
  \caption{An example flowchart. Here, the application computes three low-level features. All three combine to rate the two high-level features: blurriness and exposure. The developer controls which features are present. The photographer decides how to weight each high-level feature. UUU: (photographer's decision/opinion/input effects the default values of the highlevel features)}
  \label{fig:flowchart}
\end{figure*}

\subsection{Helping the Photographer Personalize Ratings}
\label{abstraction}
%The photographer should not have to interpret the meaning of low-level features. Instead, we use high-level features as a layer of abstraction. It is easy for a photographer to understand the meaning of high-level features, but this presents the following two questions:
UUU: (how about reordering the sentences)
The photographer should not have to interpret the meaning of low-level features, because it is easy for him to understand the meaning of high-level features. Thus, we use high-level features as a layer of abstraction.  But this presents the following two questions:

\begin{enumerate}
\item \textbf{How important is each high-level feature to the overall rating of a photograph?}

%This question is relevant to the photographer. As an example:

%Photographers Penny and Quinn want to automatically rate their photographs so they can ignore those with low ratings and save some time when browsing their collections.
For example: Photographer Penny takes pictures at concerts so she doesn't mind slightly underexposed images. Penny will decrease the default weight on the high-level "exposure" feature.

\item \textbf{How does each low-level feature affect the ranking of a high-level feature?}

%This  question is relevant to the developer. As an example:

For example: Programmer Pete calculates three low-level features per image: average width of edges, average pixel contrast, and average pixel brightness. He wants to use these low-level features to see if an image is blurry or poorly exposed.

UUU: same comment about both questions. I think we need a sentence that ties the example to the question.

\end{enumerate}

\subsection{Helping the Developer Make Progress}
\label{easeofprogramming}
%The Developer will want 
When a developer needs to add more features as research progresses. There are two types of features which can be added:
\begin{enumerate}
\item \textbf{Adding low-level features}

Say Programmer Pete calculates the number of vibrant pixels in an image. He should be able to insert this feature into the program with ease. The photographer would be unaware of any changes other than an improvement in accuracy.

\item \textbf{Adding high-level features}

Say Programmer Pete decides that Saturation is important. Then he will want to see just how much saturation is relevant to image quality. The end-user photographers will be aware of this new change, and can adjust the high-level weights accordingly.
\end{enumerate}

\section{Introduction}
To solve the problem presented, one might try to directly train low-level features to rate image quality. However, photograph quality is highly subjective and this approach does not allow a photographer to personalize the importance of each feature.

So instead, one might try to directly compute high-level features. However, this method risks using a poor algorithm to calculate a high-level feature, and thus sacrifices accuracy. (It is hard to calculate "color quality;" it is easy to calculate the number of saturated pixels.)

We thus propose a hybrid method which separately trains low-level and high-level features. This method allows (1) the photographer to personalize each high-level feature's importance, and (2) the developer to have multiple low-level calculations to rate each high-level feature. Further, the developer can (3) dynamically change which features are used (both low-level and high-level).

To answer the first question presented in Section \ref{abstraction}, we use a Linear Regression UUU (may want to add a sentence about why LG and not other technique) learning model. This allows for the photographer to adjust the importance of each feature without retraining the model. For the second question, the user does not have re-weighting control, so we use the more accurate model UUU(what model exactly and why, I know that u have it later, but since intro is very short and fuzzy, i think it is ok to be more specific here) of Support Vector Regression Machines (SVRs)\cite{springerlink:10.1023/B:STCO.0000035301.49549.88}. When the developer adds a new feature, one or both (UUU: might want to add an explanation of when it is one and when it is two. 
both if it is a low level feature.
when a photographer, aka end user, adjusts the weights, nothing
needs to be learned)
of these must get retrained. These methods are described in detail in Section \ref{methods}. The high-level features for the images in the training set are labeled by surveying Amazon Mechanical Turk\footnote{http://www.mturk.com} users (Turkers). UUU (add an image,
screeshot, of what a user sees. also, if you clean/filter the data, it worths to mention it and the reasons).

\section{Related Work}
Tong and Chang\cite{Tong:2001:SVM:500141.500159} use Support Vector Machines to search a database of images quickly by choosing images that are visually similar, which abstracts the features from the user completely.

The Personalized Photograph Ranking and Selection System\cite{Yeh:2010:PPR:1873951.1873963} computes high-level features directly and uses ListNet\cite{Cao:2007:LRP:1273496.1273513} to find a ranking. However, this does not allow for the training of several low-level features to produce a single high-level feature, which can increase the overall accuracy.
UUU: (can be expanded but since we don't have much time, it is ok to leave it as is)

\section{Data}

There are two sets of data used, both of which are learned independently.

\subsection{User-Produced Data}
\label{turkdata}
The initial set of data was produced by giving Turkers a simple statement for each high level feature, and asking if they agreed. Our primary ground-truth statement for each image is:

``This image is high quality.''

We present this to five Turkers and ask them to choose one of three options:

\begin{itemize}
\item ``Agree'' (+1 point)
\item ``Neutral'' (+.5 points)
\item ``Disagree'' (+0 points)
\end{itemize}
The points are then averaged across the five Turkers' responses to obtain a final score.

We repeat this process for each high level feature, with statements such as ``This image is in focus'' and ``This image is well-exposed.'' The data then looks as shown in Figure \ref{fig:turktable}.

\begin{figure}
\centering
\resizebox{12cm}{!}{
\begin{tabular}[t]{| c || c || c | c | c | l | }
 \hline
 & High Quality & In focus & Well exposed & Good saturation & \ldots \\ 
 \hline
Image 1 & .5 & .3 & 1 & .8 & \ldots \\ 
 \hline
Image 2 & .1 & ? & .4 & .4 & \ldots \\ 
 \hline
Image 3 & .9 & .8 & .6 & ? & \ldots \\ 
 \hline
\vdots & \vdots & \vdots & \vdots & \vdots & $\ddots$ \\
 \hline
\end{tabular}
}
\caption{The information collected from Amazon Mechanical Turk, showing example values for high-level questions. There are some values missing, indicated by a question mark (?), in case we do not have information from Turkers about that high-level feature. We discuss how to handle this in Section \ref{llfeat}.}
\label{fig:turktable}
\end{figure}

\\
\\

In this way, the primary rating for an image is learned indirectly through a linear regression of each of the other high-level features.

\subsection{Application-produced data}
The developer chooses which low-level features to calculate. Each calculation is used in an Support Vector (SVR) to get rankings of each high-level feature.
UUU (explain second sentence, how exactly it gets the ranking of a high level. yes, i know that dave knows how sv-s work, but still...)

\section{Implementation of Training Methods}
\label{methods}

There are two steps to the learning process. First, we learn the importance of each high-level feature on the overall image rating. 
UUU (mention that this is a default val of a high level fatures)
Then, we learn how each low-level feature affects each high-level feature.

\subsection{Learning High-Level Features}
We use a linear regression to train each high level feature. For this, we do not take into account any low-level features. The error function is defined as:
\[
E=\displaystyle\sum\limits_{i=0}^N R_i-Z_i
\]
Where $E$ is the error, $N$ is the number of images in the training set, $R_i$ is the overall rating obtained from Turkers for image $i$, and $Z_i$ is as follows:
\[
Z_i=\displaystyle\sum\limits_{f=0}^M w_f \cdot r_{i,f}
\]
Where $M$ is the number of high-level features, $w_f$ is the weight of feature $f$, and $r_{i,f}$ is the Turkers rating of feature $f$ on image $i$.

We minimize this error function over all $w_i$.
UUU: (should the weights sum up to 1? mention it)

\subsection{Learning Low-Level Features}
\label{llfeat}
We use an ensemble 
UUU (what is the initial number of vectors, who determines it. it is ok to reference our paper)
of SVRs to learn low-level features. Each SVR is trained to learn a single high-level feature (e.g. the last three columns of Figure \ref{fig:turktable}).
UUU (, which corresponds to a single question we ask Turkers)

We want to be able to ask Turkers new questions to use as high-level features, and add an SVR to the ensemble. If we do not retrain on every image used as training data, we need to handle missing data.

The solution is straightforward: If we do not have Turk data for a certain high-level feature, we do not train on that image for the SVR. Note that to learn high-level features, we can only use complete rows (images with no missing values).

\subsection{Output on new images}
After training, the rating $O_i$ of a new image $i$ is as follows:
\[
O_i=\frac{1}{M}\displaystyle\sum\limits_{f=0}^Ma_f \cdot w_f \cdot S_{i,f}
\]
Where $M$ is the number of high-level features, $a_f$ is the personalized weight adjustment for feature $f$, $S_{i,f}$ is the output of the Support Vector Regression for image $i$ and feature $f$, and the other variables are as defined above.

\section{Adding Features}
Here, we show the ease of adding new features in our application. Training data is saved to a file and loaded when the application begins, so training only occurs when the user requests it using our GUI (Appendix \ref{gui}).

\subsection{New High Level Features}
To add a new high level feature, the developer must:

\begin{enumerate}
\item Obtain MTurk data, as shown in Section \ref{turkdata}.
\item Place the .CSV file (provided by MTurk) in the /trainingdata/ directory.
\end{enumerate}

Then, use the GUI to retrain; the SVR and Linear Regression function are automatically updated. The application automatically combines all .CSVs in the directory and extracts labels. It handles multiple files containing high-level feature labels for the same image. See Figure \ref{fig:turktable} for the data extracted.

\subsection{New Low Level Features}
If a new value is produced by the application, it can be added to the training set with the following line of code (C++):

\texttt{features.push\_back(value);}

Where \texttt{features} is the vector (array) of low-level features and \texttt{value} is the new value. Then, use the GUI to retrain; the SVR is automatically updated.

\section{Results}
We show our accuracy by predicting on two data sets. Both of these have ground truth ratings from Turk. Because we want to show how machine learning improved 
UUU (explain what we compare to what.
reference our paper.
it can be a link to your website/github,
and say what were the results that we 
got using a,b,c features,
and what is the accuracy now)
our application, we focus on how much \textit{better} we do with machine learning than we did by guessing parameters.

By implementing machine learning, we had an added resource restriction: to train, we needed to calculate low-level features for several hundred images quickly. We had to thus remove a lot of complexity in the calculations of the low-level features, reducing the computation time from about two minutes per image to about ten seconds (on a single core Intel processor at 4.5Ghz). We show that we still improve over our untrained algorithm.

The following results are based on the default learned weights of each high level feature.
UUU (confusing sentence. or remove it or add that
no user was entered, because our reference,
our prev paper, doesn't have this option)

\section{Conclusion and Future Work}
We have developed a framework for continuing research on rating image quality. First, a developer calculates \textit{low-level feature} values based on image data. Turk users provide answers to \textit{high-level features}, as well as an overall rating of the quality.

Using linear regression, we determine which high-level features are most correlated with the overall rating. Then, we use train one Support Vector Regression per high-level feature, and using the weights from the linear regression, obtain a final value for the image.

We implement the framework in our previous research which calculates low-level features. Our accuracy increases using the machine learning techniques (as compared to arbitrarily hypothesized weights for both steps of learning: low-level to high-level and high-level to overall quality).

Future work should implement a discriminant analysis to remove irrelevant low-level features from the SVRs of high-level features, because unrelated features lower the accuracy of an SVR. For example, vibrant pixels (low-level) would have no effect on blur amount (high-level), so the extra dimension is essentially random.

An online learning algorithm could also be implemented as follows: for every image that the algorithm predicts on, automatically query MTurk for labels, and retrain the SVRs accordingly.

\subsection{Low Quality Data Set}
Our application of the framework is intended to find low quality images. Previously, out of 156 low-quality photographs, we gave 141 a low rating (less than .5). After training our algorithm, we gave XXX a low rating.

\subsection{Average Quality Data Set}
For completeness, we must also show that we do not rate high-quality and average-quality images poorly. We use INRIA's publicly-available "Original Image" dataset\cite{JDS08} for these images. Previously, we had correctly gave 135 a high rating (greater than .5). After training, we correctly gave XXX a low rating.

\appendix
\section{Graphical User Interface}
This section shows the GUI used for training and prediction.

\subsection{Training}
There are two ways to gather training data. By default, the last used SVR and Linear Regression function are loaded. The user can choose to train on new data by selecting a set of images (Figure XXX). The application looks through all the MTurk data files for matching high-level labels and trains on these images. It ignores unlabeled images, and warns the user if none of the selected images have labels.

\subsection{Prediction}
The user selects which images to predict on. Two outputs are produced: one with all images and their ranking (Figure XXX) and one with the highest-rated images(Figure XXX).

\section{CMPS 242: Notes to grader}
This paper incorporates Machine Learning into our previous work. For full disclosure, we will list what is included in this paper that was created previously:

\begin{enumerate}
\item The low-level feature calculations and related framework (but they had been optimized for speed, allowing large training datasets).
\item The GUIs in Figure XXX and XXX (but they were combined into a single interface for this paper).
\end{enumerate}

The rest of the contributions were developed solely for our final project in CMPS 242.

\bibliographystyle{plain}
\bibliography{README_BIB}

\end{document}
